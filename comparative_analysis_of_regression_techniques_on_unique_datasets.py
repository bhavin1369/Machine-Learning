# -*- coding: utf-8 -*-
"""Comparative Analysis of Regression Techniques on Unique Datasets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cyn___OpAZ2aPKUMCwovMJCwhLEiST42
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer # For handling potential missing values

# --- Configuration ---
FILE_PATH_METHANE = '/content/ethylene_methane.txt'
FILE_PATH_CO = '/content/ethylene_CO.txt'

# --- 1. Create Mock Data (Replace with your actual file loading) ---
# Assuming 'Ethylene' is the target, 'Methane'/'CO' are key features, plus others.
# Let's simulate some relationships.

# Mock Data for ethylene_methane.txt
np.random.seed(42)
num_samples = 100

data_methane = {
    'Methane_Concentration': np.random.rand(num_samples) * 100 + 50, # 50-150 ppm
    'Temperature_C': np.random.rand(num_samples) * 30 + 10,       # 10-40 C
    'Humidity_Percent': np.random.rand(num_samples) * 50 + 30,    # 30-80 %
    'Pressure_kPa': np.random.rand(num_samples) * 20 + 90,        # 90-110 kPa
    'Ethylene_Concentration': (
        5 + 0.8 * (np.random.rand(num_samples) * 100 + 50)  # Methane effect
        + 0.5 * (np.random.rand(num_samples) * 30 + 10)  # Temperature effect
        - 0.2 * (np.random.rand(num_samples) * 50 + 30)  # Humidity effect
        + np.random.randn(num_samples) * 5 # Noise
    )
}
df_methane = pd.DataFrame(data_methane)
# Introduce some missing values for demonstration
df_methane.loc[df_methane.sample(frac=0.05).index, 'Methane_Concentration'] = np.nan
df_methane.loc[df_methane.sample(frac=0.03).index, 'Temperature_C'] = np.nan

print("--- Mock df_methane Head ---")
print(df_methane.head())
print("\n--- Mock df_methane Info ---")
df_methane.info()
print("\n")

# Mock Data for ethylene_CO.txt
data_co = {
    'CO_Concentration': np.random.rand(num_samples) * 20 + 1,    # 1-21 ppm
    'Light_Intensity_Lux': np.random.rand(num_samples) * 1000 + 100, # 100-1100 Lux
    'Flow_Rate_LPM': np.random.rand(num_samples) * 5 + 1,         # 1-6 LPM
    'Ethylene_Concentration': (
        2 + 1.2 * (np.random.rand(num_samples) * 20 + 1)  # CO effect
        + 0.01 * (np.random.rand(num_samples) * 1000 + 100) # Light effect
        + np.random.randn(num_samples) * 3 # Noise
    )
}
df_co = pd.DataFrame(data_co)
# Introduce some missing values for demonstration
df_co.loc[df_co.sample(frac=0.07).index, 'CO_Concentration'] = np.nan

print("--- Mock df_co Head ---")
print(df_co.head())
print("\n--- Mock df_co Info ---")
df_co.info()
print("\n")


# --- Function to process and model each dataset ---
def analyze_dataset(df, dataset_name, target_column='Ethylene_Concentration'):
    print(f"\n--- Analyzing Dataset: {dataset_name} ---")

    # --- 1. Data Understanding & Preprocessing ---
    print("\n1. Data Understanding & Preprocessing")
    print("\nInitial Data Info:")
    df.info()
    print("\nInitial Data Description:")
    print(df.describe())
    print("\nMissing Values Before Imputation:")
    print(df.isnull().sum())

    # Drop rows where the target variable is missing (critical for training)
    df.dropna(subset=[target_column], inplace=True)

    # Impute missing values for features (using mean for numerical)
    # This imputer will be fit on training data and transform train/test
    imputer = SimpleImputer(strategy='mean')
    # Identify numerical columns for imputation, excluding the target if it's already handled
    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
    if target_column in numerical_cols:
        numerical_cols.remove(target_column)

    # Apply imputer to the numerical columns
    df[numerical_cols] = imputer.fit_transform(df[numerical_cols])

    print("\nMissing Values After Imputation:")
    print(df.isnull().sum())

    # Visualize distributions
    plt.figure(figsize=(15, 5))
    for i, col in enumerate(df.drop(columns=[target_column]).columns):
        plt.subplot(1, len(df.drop(columns=[target_column]).columns), i + 1)
        sns.histplot(df[col], kde=True)
        plt.title(f'Distribution of {col}')
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(8, 6))
    sns.histplot(df[target_column], kde=True)
    plt.title(f'Distribution of {target_column}')
    plt.show()

    # Correlation Matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
    plt.title(f'Correlation Matrix for {dataset_name}')
    plt.show()

    # Define features (X) and target (y)
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"\nTrain set size: {X_train.shape[0]} samples")
    print(f"Test set size: {X_test.shape[0]} samples")

    # Feature Scaling (Standardization)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Convert scaled arrays back to DataFrames for easier handling if needed (optional)
    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)


    # --- 2. Model Implementation & 3. Performance Evaluation ---
    print("\n2. Model Implementation & 3. Performance Evaluation")

    models = {
        'Linear Regression': LinearRegression(),
        'Lasso Regression': Lasso(alpha=0.1, random_state=42), # Alpha is a hyperparameter
        'Ridge Regression': Ridge(alpha=1.0, random_state=42), # Alpha is a hyperparameter
        'Decision Tree Regressor': DecisionTreeRegressor(random_state=42, max_depth=5), # max_depth is a hyperparameter
        'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=8) # n_estimators, max_depth are hyperparameters
    }

    results = {}

    for name, model in models.items():
        print(f"\n--- Training {name} ---")
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)

        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)

        results[name] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}

        print(f"{name} Performance:")
        print(f"  MAE: {mae:.4f}")
        print(f"  MSE: {mse:.4f}")
        print(f"  RMSE: {rmse:.4f}")
        print(f"  R-squared: {r2:.4f}")

        # Plot Actual vs. Predicted values
        plt.figure(figsize=(8, 6))
        sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        plt.xlabel("Actual Values")
        plt.ylabel("Predicted Values")
        plt.title(f'{name}: Actual vs. Predicted ({dataset_name})')
        plt.grid(True)
        plt.show()

        # Plot Residuals
        residuals = y_test - y_pred
        plt.figure(figsize=(8, 6))
        sns.scatterplot(x=y_pred, y=residuals, alpha=0.6)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel("Predicted Values")
        plt.ylabel("Residuals")
        plt.title(f'{name}: Residual Plot ({dataset_name})')
        plt.grid(True)
        plt.show()


    # --- 4. Analysis & Insights ---
    print("\n4. Analysis & Insights")
    results_df = pd.DataFrame(results).T
    print("\n--- All Model Performance Summary ---")
    print(results_df.sort_values(by='R2', ascending=False))

    best_model_name = results_df['R2'].idxmax()
    print(f"\nBest performing model based on R-squared: {best_model_name}")

    # Feature Importance (for tree-based models)
    if best_model_name in ['Decision Tree Regressor', 'Random Forest Regressor']:
        print(f"\n--- Feature Importance for {best_model_name} ---")
        model = models[best_model_name]
        feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
        print(feature_importances)

        plt.figure(figsize=(10, 6))
        sns.barplot(x=feature_importances.values, y=feature_importances.index)
        plt.title(f'Feature Importance for {best_model_name} ({dataset_name})')
        plt.xlabel('Importance')
        plt.ylabel('Feature')
        plt.show()
    elif best_model_name in ['Linear Regression', 'Lasso Regression', 'Ridge Regression']:
        print(f"\n--- Coefficients for {best_model_name} ---")
        model = models[best_model_name]
        coefficients = pd.Series(model.coef_, index=X.columns).sort_values(ascending=False)
        print(coefficients)

        plt.figure(figsize=(10, 6))
        sns.barplot(x=coefficients.values, y=coefficients.index)
        plt.title(f'Feature Coefficients for {best_model_name} ({dataset_name})')
        plt.xlabel('Coefficient Value')
        plt.ylabel('Feature')
        plt.show()

    print("\n--- General Insights ---")
    print(f"The {best_model_name} generally performed best for {dataset_name} based on R-squared.")
    print("Consider further hyperparameter tuning (e.g., using GridSearchCV or RandomizedSearchCV) for each model to optimize performance.")
    print("The correlation matrix and feature importance plots give insights into which factors are most influential on Ethylene concentration.")
    print("Residual plots can indicate if the model is missing any patterns (e.g., non-linearity). A good residual plot should show no discernible pattern.")

    print(f"\n--- End of Analysis for {dataset_name} ---")

# --- Run Analysis for each dataset ---

# Step 1: Replace mock data with actual file loading
# For ethylene_methane.txt
try:
    # Assuming your file is space-separated, comma-separated, or tab-separated
    # Adjust `sep` parameter if your delimiter is different
    df_methane_actual = pd.read_csv(FILE_PATH_METHANE, sep='\s+') # Example for space-separated
    print(f"\nSuccessfully loaded {FILE_PATH_METHANE}")
    analyze_dataset(df_methane_actual.copy(), "Ethylene_Methane Dataset") # Use .copy() to avoid modifying original df
except FileNotFoundError:
    print(f"\nWarning: {FILE_PATH_METHANE} not found. Using mock data for Ethylene_Methane analysis.")
    analyze_dataset(df_methane.copy(), "Ethylene_Methane Dataset")
except Exception as e:
    print(f"\nError loading {FILE_PATH_METHANE}: {e}. Using mock data for Ethylene_Methane analysis.")
    analyze_dataset(df_methane.copy(), "Ethylene_Methane Dataset")


# For ethylene_CO.txt
try:
    df_co_actual = pd.read_csv(FILE_PATH_CO, sep='\s+') # Example for space-separated
    print(f"\nSuccessfully loaded {FILE_PATH_CO}")
    analyze_dataset(df_co_actual.copy(), "Ethylene_CO Dataset")
except FileNotFoundError:
    print(f"\nWarning: {FILE_PATH_CO} not found. Using mock data for Ethylene_CO analysis.")
    analyze_dataset(df_co.copy(), "Ethylene_CO Dataset")
except Exception as e:
    print(f"\nError loading {FILE_PATH_CO}: {e}. Using mock data for Ethylene_CO analysis.")
    analyze_dataset(df_co.copy(), "Ethylene_CO Dataset")