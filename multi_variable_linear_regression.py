# -*- coding: utf-8 -*-
"""Multi-variable Linear Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-puqEo2QqAuordEwDL7iJIC65dBH0Z21
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

columnames=['area','rooms','price']
dataset=pd.read_csv('https://raw.githubusercontent.com/nishithkotak/machine-learning/refs/heads/master/ex1data2.txt',names=columnames)
dataset

dataset.describe()

area=dataset.iloc[0:dataset.shape[0],0:1]
rooms=dataset.iloc[0:dataset.shape[0],1:2]
price=dataset.iloc[0:dataset.shape[0],2:3]

dataset.shape

#funcation normalization
def feature_normalization(x):
  meaan=np.mean(x,axis=0)
  std=np.std(x,axis=0)
  x_normalized =(x-meaan)/std
  return x_normalized,meaan,std

data_norm=dataset.values
m=data_norm.shape[0]
#taking feature vector

x2=data_norm[:,0:2].reshape(m,2)
x2_norm,mean,std=feature_normalization(x2)

y2=data_norm[:,2:3].reshape(m,1)
x2_norm

theta_array=np.zeros((3,1))

#linear regression
def hypothesis(theta_array,x1,x2):
  return theta_array[0]+theta_array[1]*x1 +theta_array[2]*x2

def cost_funtion(theta_array,x1,x2,y,m):
  sum=0
  for i in range(m):
    sum=sum+((theta_array[0]+theta_array[1]*x1[i]+theta_array[2]*x2[i])-y[i])*2

  return sum/(2*m)

def gradiant_descent(theta_array,x1,x2,y,m,alpha):
  summation_0=0
  summation_1=0
  summation_2=0
  for i in range(m):
    summation_0=summation_0+((theta_array[0]+theta_array[1]*x1[i]+theta_array[2]*x2[i])-y[i])
    summation_1=summation_1+x1[i]*((theta_array[0]+theta_array[1]*x1[i]+theta_array[2]*x2[i])-y[i])
    summation_2=summation_2+x2[i]*((theta_array[0]+theta_array[1]*x1[i]+theta_array[2]*x2[i])-y[i])
  new_theta0=theta_array[0]-(alpha/m)*summation_0
  new_theta1=theta_array[1]-(alpha/m)*summation_1
  new_theta2=theta_array[2]-(alpha/m)*summation_2
  updated_new_theta=[new_theta0,new_theta1,new_theta2]
  return updated_new_theta

def training(x1,x2,y,alpha,iters,theta_array,m):
  theta0=0
  theta1=0
  theta2=0
  cost_values=[]
  theta_array=[theta0,theta1,theta2]
  for i in range(iters):
    theta_array=gradiant_descent(theta_array,x1,x2,y,m,alpha)
    cost_values.append(cost_funtion(theta_array,x1,x2,y,m))

  return cost_values,theta_array

alpha=0.0001
itres=50

cost_values,theta_array=training(x2_norm[:,0:1],x2_norm[:,1:2],y2,alpha,itres,theta_array,m)
x_axis=np.arange(0,len(cost_values[0]),step=1)
plt.plot(x_axis,cost_values[0])
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.title("show")
plt.show()
print(theta_array)

