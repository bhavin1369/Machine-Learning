# -*- coding: utf-8 -*-
"""activation_funcations

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pf2dSZhAz_JIXjuMLF3XoQaMhGLbBM1t
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
  return (1/(1+np.exp(-x)))

x=np.linspace(-10,10,200)
def plot_funcation(x,y,title):
  plt.plot(x,y,label=title)
  plt.title(title)
  plt.show()

plot_funcation(x,sigmoid(x),"sigmoid")

def step(x):
  return np.where(x>=0,1,0)

plot_funcation(x,step(x),"step")

def relu(x):
  return np.maximum(0,x)

plot_funcation(x,relu(x),"relu")

def linear(x):
  return x

plot_funcation(x,linear(x),"linear")

def tanh(x):
  return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))

plot_funcation(x,tanh(x),"tanh")

def softmax(x):
  e1 = np.exp(x - np.max(x))
  return e1 / np.sum(e1, axis=0)

z=np.array([0.51,0.48,0.01])
softmax(z)

plot_funcation(x,softmax(x),"softmax")

#design all gate,or,nor,and,nand,not,xor and xnor gates